[core]
dags_folder = /opt/airflow/dags
base_log_folder = /opt/airflow/logs
plugins_folder = /opt/airflow/plugins
executor = LocalExecutor
load_examples = False
default_timezone = utc
# Define users and their roles here
simple_auth_manager_users = admin:admin,dxuser:viewer
# Configure the file to store passwords (optional, but good to have)
simple_auth_manager_passwords_file = /opt/airflow/simple_auth_manager_passwords.json

[webserver]
authenticate = True
auth_backend = airflow.www.auth.backends.simple_auth_manager.SimpleAuthManager
web_server_host = 0.0.0.0
web_server_port = 8080
base_url = http://localhost:8082

[api]
auth_backends = airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session

[logging]
base_log_folder = /opt/airflow/logs
remote_logging = False

[api_auth] 
jwt_secret = ${AIRFLOW_JWT_SECRET}

[database]
sql_alchemy_conn = mysql+mysqldb://hive:${HIVE_DB_PASSWORD}@dx-database:3306/airflow

[workers]
# Kill worker processes after they complete N tasks
# (works in LocalExecutor too, because tasks spawn in workers)
worker_max_tasks_per_child = 1

# Restart workers after memory leak risk
worker_max_memory_per_child = 512m

[connections]
# Hive connection
hive_default = hive://@dx-master:10000

# Spark connection  
spark_default = spark://dx-master:7077

# HDFS connection
hdfs_default = hdfs://dx-master:8020

[scheduler]
standalone_dag_processor = True

# Prevent concurrent Spark sessions
[spark]
# Limit concurrent Spark tasks
spark_app_name_template = airflow_spark_{{ dag.dag_id }}_{{ task.task_id }}_{{ ts_nodash }}
spark_binary = spark-submit
spark_submit_hook_log_level = INFO

# Environment variables for Spark
[spark_env]
SPARK_HOME = /opt/airflow/scrips/conf/spark
HADOOP_HOME = /opt/airflow/scrips/conf/hadoop
HIVE_HOME = /opt/airflow/scrips/conf/hive
JAVA_HOME = /usr/lib/jvm/java-1.8.0-openjdk
PYSPARK_PYTHON = python3.11